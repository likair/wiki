{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to Wiki Pages of Information Technology and Communication Sciences at Tampere University.</p> <p>This wiki is meant to be a tool for personnel of our department, supporting research and teaching needs. You (yes you!) can contribute content to this wiki and help us building the common crowdsourced knowledge database.</p>"},{"location":"#university-bureaucracy","title":"University Bureaucracy","text":"<ul> <li>Contracts and salary</li> <li>MSc thesis</li> <li>PhD thesis</li> <li>CS Curricula</li> </ul>"},{"location":"#technical-notes","title":"Technical Notes","text":"<ul> <li>How to set up Remote Access</li> <li>How to install TUNI VPN</li> <li>TUNI Narvi Cluster</li> <li>Distributed data parallel training using Pytorch on the multiple nodes of CSC and Narvi</li> <li>Version control</li> <li>Wireless connections</li> </ul>"},{"location":"#meta","title":"Meta","text":"<ul> <li>How to contrubute to the Wiki</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Computer Vision Group</li> <li>Tampere University</li> </ul>"},{"location":"Meta/how-to-contribute/","title":"How to Contribute to This Wiki?","text":"<p>This Wiki \ud83d\udcd6 is a crowdsourced base of knowledge accumulated by generations of current and ex research assistants, students, faculty members, friends, and other personnel. It is \"whatever\" annoying you don't remember after two months anymore or what frequently someone asks.</p> <p>The knowledge is expected to be contributed via Markdown files (<code>.md</code>) and submitted as Pull Requests to TUNI-ITC/wiki. This guide describes this process step-by-step.</p> <p>There are two options on how to contribute to Wiki:</p> <ol> <li>Do-it-yourself \ud83d\udc4d: you are following this guide and submitting the Pull Request.</li> <li>Ask somebody: for instance, you are not familiar with <code>git</code> and don't want to discover its beauty, then, create an Issue in TUNI-ITC/wiki and share a <code>.md</code> file with us. Here are the guide and an online Markdown editor.</li> </ol> <p>What Can I Contribute?</p> <p>Anything! Style, spelling, new knowledge, Wiki's engine. Anything, ok? \ud83e\udd17</p>"},{"location":"Meta/how-to-contribute/#what-should-i-do","title":"What Should I Do?","text":"I already have an old fork on my GitHub <p>A good pratice is to have an even (up-to-date) fork with the upstream repo (<code>TUNI-ITC/wiki</code>) before you form a pull request. Otherwise, you may purpose an edit to the old content that is no longer there.</p> <p>Of course, the easiest way to sync a fork (and the last resort measure if something went wrong) is to delete the fork from your GitHub account and fork TUNI-ITC/wiki again. A less barbaric approach is to sync it. Here is how to do it.</p> <p>If you already made some new contributions, make sure to backup them to avoid issues. You may also want to remove the fork from your local machine and clone the fork again from your profile on GitHub.</p> <p>Synchronizing a fork is a simple procedure but requires you to run several lines in your terminal: <pre><code># (this line is for the first time only) Add the upsteam to your git folder.\ngit remote add upstream https://github.com/TUNI-ITC/wiki.git\ngit fetch upstream\ngit checkout main\n# WARNING: this erases all differences between upstream and your local version\ngit reset --hard upstream/main\n# pushes the changes to your fork on GitHub\ngit push origin main --force\n</code></pre></p> <p>The pipeline boils down to these several steps which should be familiar to anyone who worked with an open-source project on some sort of Hub \ud83e\udd13 (GitHub, Bitbucket, GitLab, and such):</p> <ol> <li>Create an Issue (skip if feeling lazy or playing like a bad boy)</li> <li>Fork &amp; Clone</li> <li>Add/Change</li> <li>Push to the Fork</li> <li>Create a Pull Request</li> </ol>"},{"location":"Meta/how-to-contribute/#1-create-an-issue","title":"1. Create an Issue","text":"<p>This is completely optional and we would like to see an issue just to track the progress and maybe suggest where (which section) to put your knowledge or help you with something.</p>"},{"location":"Meta/how-to-contribute/#2-fork-clone","title":"2. Fork &amp; Clone","text":"<p>You can edit directly through the GitHub web page (\"Add File\" and \"Edit\" button) or do it locally on your computer. We recommend learning to do it the hard way by forking and cloning it manually you will benefit from this knowledge in your career, otherwise skip this step.</p> <p>Fork means that you make a personal copy of this wiki - note that anyone can do that and it does not mess the main branch! To do that go to TUNI-ITC/wiki and press the top-right button Fork - if you don't have a Github account yet, you need to make one.</p> <p>After that, you should have the forked repo somewhere in your account, e.g., <code>github.com/&lt;MY_ACCOUNT&gt;/wiki/</code>.</p> <p>To clone your fork locally, in terminal type: <pre><code>git clone https://github.com/&lt;MY_ACCOUNT&gt;/wiki/\n</code></pre> and it's done!</p>"},{"location":"Meta/how-to-contribute/#3-addchange","title":"3. Add/Change","text":"<p>You may freely edit an existing file or create new, e.g., <code>how-to-select-a-coffee-in-a-finnish-supermarket.md</code>. Here are the guide and an online Markdown editor for you to play with.</p> <p>We are using Material Theme for MkDocs. Hence, you may also propose to add more functionality to our wiki. Check out the manuals of both to see what else we can add.</p> Can I check locally how it will look? <p>Sure thing! You will only need to install the <code>mkdocs-material</code> python package: <pre><code>pip install mkdocs-material\n</code></pre> Once done, you can start a preview server locally <pre><code>cd /path/to/wiki\nmkdocs serve\n</code></pre> After, just type <code>localhost:8000</code> in your browser.</p> <p>See more in the original documentation.</p>"},{"location":"Meta/how-to-contribute/#4-push-to-the-changes-to-the-fork","title":"4. Push to the Changes to the Fork","text":"<p>Commit your changes! You know how, right?</p> Ok, here is how <pre><code># it will print the modifications were made compared to last commit\ngit status\n# this will `stage` you changes\ngit add how-to-do-something.md\n# this will commit the staged changes\n# (it may ask you to configure git if you are doing to for the first time)\ngit commit -m \"added how-to-do-something.md\"\ngit push\n</code></pre>"},{"location":"Meta/how-to-contribute/#5-create-a-pull-request","title":"5. Create a Pull Request","text":"<p>Open the page with your fork on GitHub: <code>github.com/&lt;MY_ACCOUNT&gt;/wiki/</code>. At this point, you should be able to find the changes you made in your fork. Somewhere at the top, you will be asked if you want to make a Pull Request and that your branch is ahead of the <code>main</code> by some commits. Make the request, by adding comments, title, and check that you are proposing the files you expect and submit it. Someone will review and accept it. That's it!</p> <p>Note, once the PR is submitted it cannot be deleted even by the moderators.</p>"},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/","title":"Distributed data parallel training using Pytorch on the multiple nodes of CSC and Narvi clusters","text":""},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Motivation</li> <li>Outline</li> <li>Setting up a PyTorch model without DistributedDataParallel</li> <li>Setting up the same model with DistributedDataParallel</li> <li>DistributedDataParallel as a Batch job in the servers</li> <li>Tips and Tricks</li> <li>Acknowledgements</li> </ol>"},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/#motivation","title":"Motivation","text":"<p>Training a Deep Neural Network (DNNs) is notoriously time-consuming especially nowadays when they are getting bigger to get better. To reduce the training time, we mostly train it on the multiple gpus within a single node or across different nodes. This tutorial is focused on the latter where multiple nodes are utilised using PyTorch. Although there are many tutorials available on the web including one from the PyTorch, they are not self-sufficient in explaining some of the key issues like how to run the code, how to save checkpoints, or how to create a batch script for this in the severs. I have given a starter kit here which addresses these issues and can be helpful to students of our university in setting up their first multi-gpu training in the servers like CSC-Puhti or Narvi.</p>"},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/#outline","title":"Outline","text":"<p>PyTorch mostly provides two functions namely <code>nn.DataParallel</code> and <code>nn.DistributedDataParallel</code> to use multiple gpus in a single node and multiple nodes during the training respectively. However, it is recommended by PyTorch to use <code>nn.DistributedDataParallel</code> even in the single node to train faster than the <code>nn.DataParallel</code>. For more details, I would recommend reading the PyTorch docs. This tutorial assumes that the reader is familiar with the DNNs training using PyTorch and basic operations on the gpu-servers of our university.</p>"},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/#setting-up-a-pytorch-model-without-distributeddataparallel","title":"Setting up a PyTorch model without DistributedDataParallel","text":"<p>I have considered a simple Auto-Encoder (AE) model for demonstration where the inputs are images of digits from MNIST data-set. Just to be clear, AE takes images as input and encodes it to a much smaller dimension w.r.t its inputs and then try to reconstruct the images back from those smaller dimensions. It can be considered as a process of compression and decompression. We train the network to learn this smaller dimension such that the reconstructed image is very close to input. Let's begin by defining the network structure.</p> <p><pre><code>import torch\nimport torch.nn as nn\nimport torchvision\nfrom argparse import ArgumentParser\n\nclass AE(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(in_features=kwargs[\"input_shape\"], out_features=128),\n            nn.ReLU(inplace=True),\n            # small dimension\n            nn.Linear(in_features=128, out_features=128),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=128, out_features=128),\n            nn.ReLU(inplace=True),\n            # Recconstruction of input\n            nn.Linear(in_features=128, out_features=kwargs[\"input_shape\"]),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, features):\n        reconstructed = self.net(features)\n        return reconstructed\n</code></pre> Lets create a <code>train()</code> function where we load the MNIST data-set and this can easily be done from the <code>torchvision.dataset</code> library as follows <pre><code>def train(gpu, args):\n    transform = torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor()\n    ])\n\n    train_dataset = torchvision.datasets.MNIST(\n        root=\"~/mnist_dataset\", train=True, transform=transform, download=True\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=128, shuffle=True, num_workers=4,\n        pin_memory=True\n    )\n</code></pre> Transfer the model to the GPU now and declare the optimiser and loss criterion for the training process. <pre><code>def train(gpu, args):\n    transform = torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor()\n    ])\n\n    train_dataset = torchvision.datasets.MNIST(\n        root=\"./mnist_dataset\", train=True, transform=transform, download=True\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=128, shuffle=True, num_workers=4,\n        pin_memory=True\n    )\n\n    # load the model to the specified device, gpu-0 in our case\n    model = AE(input_shape=784).cuda(gpu)\n    # create an optimizer object\n    # Adam optimizer with learning rate 1e-3\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # Loss function\n    criterion = nn.MSELoss()\n</code></pre> Now wrap everything in the training function and start training</p> <p><pre><code>def train(gpu, args):\n    transform = torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor()\n    ])\n\n    train_dataset = torchvision.datasets.MNIST(\n        root=\"./mnist_dataset\", train=True, transform=transform, download=True\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=128, shuffle=True, num_workers=4,\n        pin_memory=True\n    )\n\n    # load the model to the specified device, gpu-0 in our case\n    model = AE(input_shape=784).cuda(gpu)\n    # create an optimizer object\n    # Adam optimizer with learning rate 1e-3\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # Loss function\n    criterion = nn.MSELoss()\n\n    for epoch in range(args.epochs):\n        loss = 0\n        for batch_features, _ in train_loader:\n            # reshape mini-batch data to [N, 784] matrix\n            # load it to the active device\n            batch_features = batch_features.view(-1, 784).cuda(gpu)\n\n            # reset the gradients back to zero\n            # PyTorch accumulates gradients on subsequent backward passes\n            optimizer.zero_grad()\n\n            # compute reconstructions\n            outputs = model(batch_features)\n\n            # compute training reconstruction loss\n            train_loss = criterion(outputs, batch_features)\n\n            # compute accumulated gradients\n            train_loss.backward()\n            # pe-rform parameter update based on current gradients\n            optimizer.step()\n\n            # add the mini-batch training loss to epoch loss\n            loss += train_loss.item()\n\n            # compute the epoch training loss\n        loss = loss / len(train_loader)\n\n        # display the epoch training loss\n        print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch+1, args.epochs, loss))\n</code></pre> Now lets finish this code with a <code>main()</code> function that calls the train function and defines the required arguments. <pre><code>def main():\n    parser = ArgumentParser()\n    parser.add_argument('--ngpus', default=1, type=int,\n                        help='number of gpus per node')\n\n    parser.add_argument('--epochs', default=2, type=int, metavar='N',\n                        help='number of total epochs to run')\n    args = parser.parse_args()\n    train(0, args)\n\nif __name__ == '__main__':\n    main()\n</code></pre></p>"},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/#setting-up-the-same-model-with-distributeddataparallel","title":"Setting up the same model with DistributedDataParallel","text":"<p>With the multiprocessing, we will run our training script in each node separately and ask PyTorch to handle the synchronisation between them. It makes sure that in each iteration, the same network weights are present in every node but use different data for the forward pass. Then the gradients are accumulated from every node to calculate the change in weights which will be sent to each node for the update. In short, the same network operates on different data in different nodes in parallel to make things faster. To let this internal communication happen between the nodes, we need few information to setup the DistributedParallel environment such as 1. how many nodes we are using, 2. what is the ip-address of the master node and 3. The number of gpus in a single node. I have changed the order of the above code to make it more understandable. We will first start from the <code>main</code> function by defining all the necessary variables.</p> <ul> <li>A single node can be understood as a single computer with its own gpus and cpus. Here we need multiple of such computers. One thing to remember is that these nodes should be connected to each other. In the servers, they are always connected to each other so we can use it without any problems. In the script, we need to mention the ip-address and port of one of the nodes (we call it the master node) so that all other nodes can be connected to that automatically when we start the script in those nodes.</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom argparse import ArgumentParser\nimport os\n\nif __name__ == \"__main__\":\n\n    parser = ArgumentParser()\n    parser.add_argument('--nodes', default=1, type=int)\n    parser.add_argument('--local_ranks', default=0, type=int,\n                        help=\"Node's order number in [0, num_of_nodes-1]\")\n    parser.add_argument('--ip_adress', type=str, required=True,\n                        help='ip address of the host node')\n    parser.add_argument(\"--checkpoint\", default=None,\n                        help=\"path to checkpoint to restore\")\n    parser.add_argument('--ngpus', default=1, type=int,\n                        help='number of gpus per node')\n    parser.add_argument('--epochs', default=2, type=int, metavar='N',\n                        help='number of total epochs to run')\n\n    args = parser.parse_args()\n    # Total number of gpus availabe to us.\n    args.world_size = args.ngpu * args.nodes\n    # add the ip address to the environment variable so it can be easily avialbale\n    os.environ['MASTER_ADDR'] = args.ip_adress\n    print(\"ip_adress is\", args.ip_adress)\n    os.environ['MASTER_PORT'] = '8888'\n    os.environ['WORLD_SIZE'] = str(args.world_size)\n    # nprocs: number of process which is equal to args.ngpu here\n    mp.spawn(train, nprocs=args.ngpus, args=(args,))\n</code></pre> <ul> <li>You can imagine the <code>local_rank</code> as an unique number associated to each node starting from zero to number of nodes-1. We assign zero rank to the node whose ip-address is passed to the <code>main()</code> and we start the script first on that node. Further, we are going use this number to calculate one more rank for each gpu in that node.</li> <li>Instead of calling the <code>train</code> function once, we spawn <code>args.ngpus</code> processes in each node to run <code>args.ngpus</code> instances of <code>train</code> function in parallel.</li> </ul> <p>Now lets define the function <code>train</code> that can handle these multiple processes.</p> <pre><code>def train(gpu, args):\n\n    args.gpu = gpu\n    print('gpu:',gpu)\n    # rank calculation for each process per gpu so that they can be identified uniquely.\n    rank = args.local_ranks * args.ngpus + gpu\n    print('rank:',rank)\n    # Boilerplate code to initialize the parallel prccess.\n    # It looks for ip-address and port which we have set as environ variable.\n    # If you don't want to set it in the main then you can pass it by replacing\n    # the init_method as ='tcp://&lt;ip-address&gt;:&lt;port&gt;' after the backend.\n    # More useful information can be found in\n    # https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html\n\n    dist.init_process_group(\n        backend='nccl',\n        init_method='env://',\n        world_size=args.world_size,\n        rank=rank\n    )\n    torch.manual_seed(0)\n    # start from the same randomness in different nodes. If you don't set it\n    # then networks can have different weights in different nodes when the\n    # training starts. We want exact copy of same network in all the nodes.\n    # Then it will progress from there.\n\n    # set the gpu for each processes\n    torch.cuda.set_device(args.gpu)\n\n\n    transform = torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor()\n    ])\n\n    train_dataset = torchvision.datasets.MNIST(\n        root=\"~/mnist_dataset\", train=True, transform=transform, download=True\n    )\n    # Ensures that each process gets differnt data from the batch.\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset, num_replicas=args.world_size, rank=rank\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        # calculate the batch size for each process in the node.\n        batch_size=int(128/args.ngpus),\n        shuffle=(train_sampler is None),\n        num_workers=4,\n        pin_memory=True,\n        sampler=train_sampler\n    )\n</code></pre> <ul> <li>As we are going to submit the training script to each node separately, we need to set a random seed to fix the randomness involved in the code. For example, in the very first iteration the network weights will start from the same random weights (seed=0) in the different nodes. Then PyTorch will handle the synchronisation and at the end of training, we will have the same network weights in each node.</li> <li><code>train_sampler</code>, <code>manual_seed</code> and <code>modified batch size in the dataloader</code> are important steps to remember while setting this up.</li> </ul> <p>Finally, wrap the model as DistributedDataParallel and start the training. <pre><code>def train(gpu, args):\n    args.gpu = gpu\n    print('gpu:',gpu)\n    rank = args.local_ranks * args.ngpus + gpu\n    # rank calculation for each process per gpu so that they can be\n    # identified uniquely.\n    print('rank:',rank)\n    # Boilerplate code to initialise the parallel process.\n    # It looks for ip-address and port which we have set as environ variable.\n    # If you don't want to set it in the main then you can pass it by replacing\n    # the init_method as ='tcp://&lt;ip-address&gt;:&lt;port&gt;' after the backend.\n    # More useful information can be found in\n    # https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html\n\n    dist.init_process_group(\n        backend='nccl',\n        init_method='env://',\n        world_size=args.world_size,\n        rank=rank\n    )\n    torch.manual_seed(0)\n    # start from the same randomness in different nodes.\n    # If you don't set it then networks can have different weights in different\n    # nodes when the training starts. We want exact copy of same network in all\n    # the nodes. Then it will progress form there.\n\n    # set the gpu for each processes\n    torch.cuda.set_device(args.gpu)\n\n\n    transform = torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor()\n    ])\n\n    train_dataset = torchvision.datasets.MNIST(\n        root=\"./mnist_dataset\", train=True, transform=transform, download=True\n    )\n    # Ensures that each process gets differnt data from the batch.\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset, num_replicas=args.world_size, rank=rank\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        # calculate the batch size for each process in the node.\n        batch_size=int(128/args.ngpus),\n        shuffle=(train_sampler is None),\n        num_workers=4,\n        pin_memory=True,\n        sampler=train_sampler\n    )\n\n\n    # load the model to the specified device, gpu-0 in our case\n    model = AE(input_shape=784).cuda(args.gpus)\n    model = torch.nn.parallel.DistributedDataParallel(\n        model_sync, device_ids=[args.gpu], find_unused_parameters=True\n    )\n    # create an optimizer object\n    # Adam optimizer with learning rate 1e-3\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # Loss function\n    criterion = nn.MSELoss()\n\n    for epoch in range(args.epochs):\n        loss = 0\n        for batch_features, _ in train_loader:\n            # reshape mini-batch data to [N, 784] matrix\n            # load it to the active device\n            batch_features = batch_features.view(-1, 784).cuda(args.gpus)\n\n            # reset the gradients back to zero\n            # PyTorch accumulates gradients on subsequent backward passes\n            optimizer.zero_grad()\n\n            # compute reconstructions\n            outputs = model(batch_features)\n\n            # compute training reconstruction loss\n            train_loss = criterion(outputs, batch_features)\n\n            # compute accumulated gradients\n            train_loss.backward()\n\n            # perform parameter update based on current gradients\n            optimizer.step()\n\n            # add the mini-batch training loss to epoch loss\n            loss += train_loss.item()\n\n        # compute the epoch training loss\n        loss = loss / len(train_loader)\n\n        # display the epoch training loss\n        print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch+1, args.epochs, loss))\n        if rank == 0:\n            dict_model = {\n                'state_dict': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n                'epoch': args.epochs,\n            }\n            torch.save(dict_model, './model.pth')\n</code></pre></p> <ul> <li>Save the model only when the rank is zero because all the models are the same. We only need to save one copy of the model. If we are not careful here then all the processes will try to save weights and can corrupt the weights.</li> </ul> <p>Save the script as <code>train.py</code> in the CSC or Narvi server and submit an interactive job with two gpu nodes (Lets quickly test it on <code>gputest</code> node as <code>srun --pty --account=Project_** --nodes=2 -p gputest --gres=gpu:v100:1,nvme:100 -t 00:15:00 --mem-per-cpu=20000 --ntasks-per-node=1 --cpus-per-task=8 /bin/bash -i</code>). Once it is allocated, ssh to each node in two terminals as <code>ssh &lt;node name&gt;</code>) and submit the job by typing <code>python train.py --ip_adress=**.**.**.** --nodes 2 --local_rank 0 --ngpus 1 --epochs 1</code> and <code>python train.py --ip_adress=&lt;same as the first&gt; --nodes 2 --local_rank 1 --ngpus 1 --epochs 1</code> to each of them respectively. Two job should start with synchronisation and training will begin soon after.</p> <ul> <li>The ip-address of a node can be obtained by <code>ping &lt;node name&gt;</code></li> </ul>"},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/#distributeddataparallel-as-a-batch-job-in-the-servers","title":"DistributedDataParallel as a Batch job in the servers","text":"<p>When we are submitting the interactive jobs, we know the exact node name and can obtain the ip-address for that beforehand. However, in the batch job, it needs to be programmed to automate most of the stuff. We have to make minimum changes to the existing code and write a <code>.sh</code> script to submit the job. Our <code>train.py</code> script are modified only in the first few lines of the <code>train()</code> function as follows</p> <pre><code>def train(gpu, args):\n\n    args.gpu = gpu\n    print('gpu:',gpu)\n\n    # rank calculation for each process per gpu so that they can be\n    # identified uniquely.\nrank = int(os.environ.get(\"SLURM_NODEID\")) * args.ngpus + gpu\nprint('rank:',rank)\n    # Boilerplate code to initialise the parallel process.\n    # It looks for ip-address and port which we have set as environ variable.\n    # If you don't want to set it in the main then you can pass it by replacing\n    # the init_method as ='tcp://&lt;ip-address&gt;:&lt;port&gt;' after the backend.\n    # More useful information can be found in\n    # https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html\n\n    dist.init_process_group(\n        backend='nccl',\n        init_method='env://',\n        world_size=args.world_size,\n        rank=rank\n    )\n    torch.manual_seed(0)\n    # start from the same randomness in different nodes.\n    # If you don't set it then networks can have differnt weights in different\n    # nodes when the training starts. We want exact copy of same network in\n    # all the nodes. Then it will progress form there.\n\n    # set the gpu for each processes\n    torch.cuda.set_device(args.gpu)\n</code></pre> <ul> <li>Instead of using local rank in calculation of process rank, we use environment variable <code>$SLURM_NODEID</code> which is unique for each slurm node.</li> </ul> <p>Keeping everything else in the code same, now lets write the batch script for CSC-puhti. Same script can be used for Narvi.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=name\n#SBATCH --account=Project_******\n#SBATCH -o out.txt\n#SBATCH -e err.txt\n#SBATCH --partition=gpu\n#SBATCH --time=08:00:00\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=8000\n#SBATCH --gres=gpu:v100:4\n#SBATCH  --nodes=2\nmodule load gcc/8.3.0 cuda/10.1.168\nsource &lt;virtual environment name&gt;\n\n# if some error happens in the initialation of parallel process then you can\n# get the debug info. This can easily increase the size of out.txt.\nexport NCCL_DEBUG=INFO  # comment it if you are not debugging distributed parallel setup\n\nexport NCCL_DEBUG_SUBSYS=ALL # comment it if you are not debugging distributed parallel setup\n\n# find the ip-address of one of the node. Treat it as master\nip1=`hostname -I | awk '{print $2}'`\necho $ip1\n\n# Store the master node\u2019s IP address in the MASTER_ADDR environment variable.\nexport MASTER_ADDR=$(hostname)\n\necho \"r$SLURM_NODEID master: $MASTER_ADDR\"\n\necho \"r$SLURM_NODEID Launching python script\"\n\nsrun python train.py --nodes=2 --ngpus 4 --ip_adress $ip1 --epochs 1\n</code></pre> <p>To Narvi users</p> <p>Change the ip1=<code>hostname -I | awk '{print $2}'</code> line to ip1=<code>hostname -I | awk '{print $1}'</code> to correctly parse the ip address.</p> <p>To Mahti users</p> <p>For now add <code>export NCCL_IB_DISABLE=1</code> to the batch script to prevent the occasional hang in the training loop. However, I am not sure whether this is happening becasue of Mahti or Pytorch 1.8.</p>"},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>If you have <code>os.mkdir</code> inside the script then always wrap it with <code>try and except</code>. Multiple processes will try to create a new folder and they will throw errors that the directory already exists.</li> <li>When resuming the network weights if your model complains that the tensors are not on the same advice and points to the optimiser then it is mostly caused by this optimizer-error. Just add these few lines after loading the optimizer from the checkpoints.</li> </ul> <pre><code>for state in optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda(gpus)\n</code></pre> <ul> <li>To run on a single node with multiple gpus, just make the <code>--nodes=1</code> in the batch script.</li> <li>If you Batchnorm*d inside the network then you may consider replacing them with <code>sync-batchnorm</code> to have better batch statistics while using DistributedDataParallel.</li> <li>Use this feature when it is required to optimise the gpu usage.</li> </ul>"},{"location":"Technical-Notes/Distributed_dataparallel_pytorch/#acknowledgements","title":"Acknowledgements","text":"<p>I found this article really helpful when I was setting up my DistributedDataParallel framework. Many missing details can be found in this article which is skipped here to focus more on the practical things. If you have any suggestions then reach me at <code>soumya.tripathy@tuni.fi</code>.</p>"},{"location":"Technical-Notes/how-to-set-up-remote-access/","title":"How to Set up Remote Access","text":"<p>This is the guide for setting up the remote connection to your office machine and it will not tell you how to set up the machine itself (e.g. install Ubuntu, allocate disk space). If you would like some guidance on this topic, for example, you would want to make your remote machine to \"look\" like the rest of our machines, please refer to this unofficial guide \u2013 also, let us know if it was useful and whether the wiki might benefit from having it here.</p> <p>Warning</p> <p>Since this guide relies on <code>ssh-forward.tuni.fi</code> as a proxy server, you may only set up the remote connection if you are a staff member of the university, i.e. a student cannot apply for access to <code>ssh-forward.tuni.fi</code>. If you are a student, please contact the IT-Helpdesk and ask if they can give you access to <code>ssh-forward.tuni.fi</code>.</p> <p>A bit of motivation and how it will work. There is no official way of conneting remotely to a self-maintained machine. Here is how we can work around this problem. Your compute machine can be added to a research network <code>pit.cs.tut.fi</code> which practically means that it will have a fixed IP (or FQDN) and you will not need to type your credentials every 24 hours. However, another problem here is that this research network can only be reachable using university-maintained devices which uses a university WiFi or a pre-installed VPN. The solution to this problem is to use a proxy ssh-server (<code>ssh-forward.tuni.fi</code>) when connecting to your machine. This ssh-forwarding server is open to the public internet and from there you can reach <code>pit.cs.tut.fi</code>.</p> <p>We assume that you have an Linux desktop (host) at your office and you would like to access it remotely from any device, e.g. your laptop (client). Here we provide both: how to setup the host and the client sides. If the host is already set up and you would like to just learn how to connect to it, follow the guide for a client.</p>"},{"location":"Technical-Notes/how-to-set-up-remote-access/#how-to-set-up-the-host-eg-compute-machine","title":"How to Set up the Host (e.g. compute machine)?","text":"<ol> <li>Email <code>it-helpdesk@tuni.fi</code> and ask them to connect your office machine to <code>pit.cs.tut.fi</code> network. They will assign a fixed IP/FQDN and you will not need to type your credentials every 24 hours to have an internet connection. Specify the following information:<ul> <li>the inventory number of the machine (on the sticker),</li> <li>MAC address of the socket in the machine you would like to use for the wired connection to the internet (you may have several Ethernet ports \u2013 you need only one),</li> <li>mention the Ethernet socket number from the wall that you will use.</li> </ul> </li> <li>At this point, you should have had received the response from <code>it-helpdesk@tuni.fi</code> and be able to connect to the internet using the socket you specified. If so, check your IP and type <code>host your_IP</code> in your terminal to find out the FQDN of the machine. In our case, it was be something like <code>&lt;IP.reversed&gt; pointed to **********.pit.cs.tut.fi</code>.</li> <li>Install <code>openssh-server</code> package on your host machine (via e.g. <code>sudo apt-get install openssh-server</code>). This will allow <code>ssh</code> connection to this machine.</li> <li>Next, make sure no WiFi connection connects automatically after the startup. On Ubuntu, type <code>sudo nm-connection-editor</code> in your terminal (or just go to <code>Edit connection</code> from the status menu on Ubuntu 16.04).</li> <li>Also, allow your connection (by default called <code>Wired connection</code>) to automatically connect when available.</li> </ol> <p>Now your machine (host) should be reachable from TUNI-maintained computers connected to <code>TUNI-STAFF</code> WiFi or a pre-installed VPN directly via <code>ssh user@**********.pit.cs.tut.fi</code>. If you are using a non-university computer, you need to use the ssh-forwarding server (<code>ssh-forward.tuni.fi</code>) to reach <code>pit.cs.tut.fi</code>. Use the following guide to set up the connection to the forwarding server.</p>"},{"location":"Technical-Notes/how-to-set-up-remote-access/#how-to-set-up-the-client-eg-your-laptop","title":"How to Set up the Client (e.g. your laptop)?","text":"<p>Even though university-maintained devices can reach <code>pit.cs.tut.fi</code> without any proxy from university premises, we found that using the proxy even on university-maintained devices provides a more uniform experience. Otherwise you need to keep in mind which network you are using each time you ssh to your machine and adjust the CLI command accordingly (see the tip in the end of this section on how to configure an <code>ssh</code> command to use proxy by default when connecting to a specific device).</p> <ol> <li>To connect to your compute machine from a self-maintained/personal device, you need to get access to the ssh-forwarding server <code>ssh-forward.tuni.fi</code>. For this, proceed to id.tuni.fi/idm -&gt; <code>My user rights</code> -&gt; <code>Apply for a new user right</code> -&gt; if required, select your Staff contract -&gt; search for <code>Linux Servers (LINUX-SERVERS) Personnel SSH tunneling service</code> and select it. In <code>Application details</code> put something like <code>For pit.cs.tut.fi connections</code>. Then, go to the <code>Applications</code> tab and wait until the access is granted (1 min).</li> <li><code>ssh-forward.tuni.fi</code> is open to the public internet and it requires 2-factor authentication. For this, log in to the forwarding server using your TUNI credentials (<code>ssh tuni_user@ssh-forward.tuni.fi</code>) while being connected to one of the University networks (<code>roam.fi/eduroam/TUNI-STAFF</code> or a university VPN). If 2FA was not initialized, it will reject your password. Unfortunately, you cannot initialize 2-factor authentication from any network. You need to be physically at University and be connected to one of the networks there.</li> <li>Once you logged in to <code>ssh-forward.tuni.fi</code>, type <code>google-authenticator</code>. It will ask you several questions and show a QR code (resize your window to see it). Answer the questions as follows:<ul> <li><code>Do you want authentication tokens to be time-based...</code> -&gt; y</li> <li><code>Do you want me to update your \"/home/user/...google_authenticator\" file</code> -&gt; y</li> <li><code>Do you want to disallow multiple uses...</code> -&gt; n</li> <li><code>By default, tokens are good for 30 seconds...</code> -&gt; n</li> <li><code>If the computer that you are logging into...</code> -&gt; y</li> </ul> </li> <li>Once QR code is shown, install some 2-factor authenticator app on your smartphone if you don't have one yet e.g. from Microsoft, Authy, or Google. The app will be used for two-step authentication when you connect from a non-university network is used e.g. your home internet. Once it is done, it will create an entry with a 6-digit passcode which changes every 30 secs.</li> <li>Test your 2FA by trying to connect to <code>ssh-forward.tuni.fi</code> from a non-university network (e.g. using internet shared from your cell-phone) (<code>ssh your_tuni_username@ssh-forward.tuni.fi</code>).</li> <li>Now you can connect to your machine from your local terminal jumping through the ssh proxy. Here is a convenient command: <code>ssh -J your_tuni_username@ssh-forward.tuni.fi your_host_username@*********.pit.cs.tut.fi</code> (<code>-J</code> means \"jump\" using the specified proxy)<ul> <li>If you are on a University network (<code>roam.fi/eduroam/TUNI-STAFF</code> or VPN), it will only ask for your TUNI and host passwords;</li> <li>If you are using a non-University network, it will first ask you for a <code>Verification code</code> which is a temporal code from the 2FA app you installed on your smartphone. Then, you will need to type your TUNI and host passwords.</li> </ul> </li> </ol> <p>Tip</p> <p>Config the <code>ssh</code> connection in <code>~/.ssh/config</code>: <pre><code>Host connection_name\n  HostName ***********.pit.cs.tut.fi\n  User your_host_username\n  ProxyCommand ssh your-tuni-username@ssh-forward.tuni.fi -W %h:%p\n</code></pre> After doing this, you will be able to do <code>ssh connection_name</code> to <code>ssh</code> directly to <code>*********.pit.cs.tut.fi</code>, forward ports, and transfer large files using <code>scp/rsync</code>. It is also useful if you are using <code>VSCode</code> or any other text editor which supports remote development. Additionally, it is handy if you would like to mount folders from the host to your client. You can use <code>sshfs connection_name:/path/to/remote_folder /path/to/local_folder</code>.</p>"},{"location":"Technical-Notes/how-to-set-up-remote-access/#known-issues","title":"Known Issues","text":"<ul> <li>At the moment, only a staff member (doctorate students are staff members usually) can use <code>ssh-forward.tuni.fi</code> proxy and, therefore, benefit from this set up. Please, let us know if you found a way around it.</li> <li>You will need to contact <code>it-helpdesk@tuni.fi</code> and ask to activate and configure your wall internet socket in a special way. This might take some time.</li> <li><code>ssh-forward.tuni.fi</code> doesn't support key-pair authentication.</li> <li>Depending on the network you are using, a different log in procedure will be required: only your TUNI password if you are connected to <code>roam.fi/eduroam/TUNI-STAFF</code>; and 2FA + your TUNI and host-machine passwords  on other networks.</li> <li><code>ssh-forward.tuni.fi</code> has very limited disk space for each user (few MB). Therefore, can only be used as a proxy for your <code>ssh</code> connection which is its main purpose.</li> </ul>"},{"location":"Technical-Notes/install-tuni-vpn/","title":"Install TUNI VPN","text":"<p>The TUNI intra documentation is here https://intra.tuni.fi/en/handbook?page=2638 but its kind of a mess and therefore below easy to follow information.</p>"},{"location":"Technical-Notes/install-tuni-vpn/#self-maintained-linux-box-tested-ubuntu-1804","title":"Self-maintained Linux Box (tested Ubuntu 18.04)","text":"<p>eduVPN application is not available for Linux and thefore you must use openvpn. Generic instructions are available here https://eduvpn.tuni.fi/vpn-user-portal/documentation for various OS, but, for example, Ubuntu is described below.</p> <p>Install OpenVPN:</p> <pre><code>~$ sudo apt-get install network-manager-openvpn-gnome\n</code></pre> <p>Generate your own at https://eduvpn.tuni.fi/vpn-user-portal/configurations (give name, e.g., \"joni-laptop\", and store somewhere).</p> <p>Start openvpn:</p> <p><pre><code>~$ sudo openvpn &lt;PATH_TO_MY_GEN_CONFIG&gt;.ovpn\n</code></pre> And that's it - your internet connection goes over VPN and you have access to university computers!!</p> <p>Note: You need to update the VPN configuration files every now and then (the expriry date by default is in the filename, e.g., \"Downloads/eduvpn.tuni.fi_internet_20201102_joni-laptop.ovpn#)</p>"},{"location":"Technical-Notes/install-tuni-vpn/#update-on-the-linux-desktop-client-of-eduvpn","title":"Update on the Linux desktop client of eduVPN:","text":"<p>There is a Linux desktop client and Python API for eduVPN and here is the link to that: https://python-eduvpn-client.readthedocs.io/en/master/introduction.html#installation</p> <p>I tested it on Ubuntu 18.04 and 20.04 and it works fine. The steps are simple and listed below (taken from the above documentation link).  <pre><code>$ sudo -s\n$ apt install apt-transport-https curl\n$ curl -L https://repo.eduvpn.org/debian/eduvpn.key | apt-key add -\n$ echo \"deb https://repo.eduvpn.org/debian/ stretch main\" &gt; /etc/apt/sources.list.d/eduvpn.list\n$ apt update\n$ apt install eduvpn-client\n</code></pre> Open the eduvpn-client from the terminal (or app launcher) and add the university name from the drop-down menu. They may ask you to sign-in with the TUNI username. Then select the university name and toggle the connected button. If it does not work then you are on your own. </p>"},{"location":"Technical-Notes/tuni-narvi-cluster/","title":"TUNI Narvi Cluster","text":"<p>This amazing guide was originally posed in wiki.eduuni.fi and composed by Heikki Huttunen. We obtained his permission to use it here. It was modernized and expanded since then.</p> <p>This document describes how to use the <code>TUNI TCSC Narvi</code> computing cluster.</p> <p>What is Narvi?</p> <ul> <li>Narvi is the SLURM cluster that substituted the old merope cluster in 2017.</li> <li>The cluster is used by researchers, faculty members, and students at Tampere University.</li> <li>There are 140 CPU-only nodes with 3000+ CPU cores</li> <li>Also Narvi has 22 nodes with different GPU nodes with 4 GPUs in each. Specifically, there are<ul> <li>6 nodes with Tesla V100 32 GB and 4 with Tesla V100 16 GB</li> <li>2 nodes with Tesla P100 16 GB and 6 with Tesla P100 12 GB</li> <li>4 nodes with Tesla K80 12 GB</li> </ul> </li> </ul>"},{"location":"Technical-Notes/tuni-narvi-cluster/#how-to-get-an-account","title":"How to Get an Account?","text":"<ol> <li>Go to id.tuni.fi</li> <li>Choose <code>Identity management</code> \u2192 <code>My user rights</code> \u2192 <code>Apply for a new user right</code></li> <li>Choose the correct contract (studen/staff).<ul> <li>If you fill the application as a student also tell the Course name and the responsible teacher</li> </ul> </li> <li>Search for <code>Linux Servers (LINUX-SERVERS) TCSC HPC Cluster</code></li> <li>In the form, enter application details: <code>I need Narvi account. My supervisor is X.</code>.</li> <li>Your supervisor will receive an acceptance link and you will be granted a new account.</li> <li>In a few days, you will receive an email from TCSC telling you to send an ssh public key to them. Create a key pair (see below or just google for it) and send the public one to them.</li> <li>Soon you will be able to login to the front end <code>narvi.tut.fi</code> using <code>ssh</code>.</li> </ol> How to generate an <code>ssh</code> key-pair \ud83d\udd10? <p>Here is how to do it on Linux and Mac systems. The instructions for Windows can be easily found on google. <pre><code>ssh-keygen -f ~/.ssh/narvi_key\n</code></pre> The command will ask for a new password which will be asked when this key is used. After, the script will save two keys (<code>narvi_key</code> and <code>narvi_key.pub</code>) in <code>~/.ssh/</code> folder. <code>*.pub</code> is the public key.</p> <p>The first time you will use this key with <code>ssh</code> it may complain about permissions (<code>Permissions are too open.</code>). If so, you will need to change the permissions of the private key <pre><code>chmod 600 ~/.ssh/narvi_key\n</code></pre></p> I don't see any GPU partitions <p>Please write an e-mail to the admin (<code>tcsc.tau@tuni.fi</code>) asking to add you to the GPU group. By default, you will only have access to CPU-only nodes.</p>"},{"location":"Technical-Notes/tuni-narvi-cluster/#how-to-check-the-queue","title":"How to Check the Queue","text":"<p>To see the status of the queue, type <pre><code>squeue\n# for a specific partitions (e.g. `normal` or `gpu`).\nsqueue -p gpu\n# for a specific user\nsqueue -u &lt;user&gt;\n</code></pre></p>"},{"location":"Technical-Notes/tuni-narvi-cluster/#how-to-run-a-job","title":"How to Run a Job","text":"<p>Remember: do not use the login node for computation \u2013 it is slow and will degrade the performance of the login node for other users!</p> <p>There are two common ways to run a job at a <code>slurm</code> cluster:</p> <ul> <li><code>srun</code></li> <li><code>sbatch</code></li> </ul> <p>The main difference is that <code>srun</code> is interactive which means the terminal will be attached to the current session. The experience is just like with any other command in your terminal. Note, that when the queue is full you will have to wait until you get resources.</p> <p>If you use <code>sbatch</code>, you submit your job to the slurm queue and get your terminal back; you can disconnect, kill your terminal, etc. with no consequence. In the case of <code>srun</code>, killing the terminal would kill the job. Hence, <code>sbatch</code> is recommended.</p> <p>Here is the example <code>srun</code> command which will ask the cluster to start an interactive shell with 1 GPU (<code>--gres</code>) and 10 CPUs (<code>--cpus-per-task</code>), 10 GB of RAM (<code>--mem-per-cpu</code>) that will be available to you for 30 minutes (<code>--time</code>): <pre><code>srun \\\n    --pty \\\n    --job-name pepe_run \\\n    --partition gpu \\\n    --gres gpu:1 \\\n    --mem-per-cpu 1G \\\n    --ntasks 1 \\\n    --cpus-per-task 10 \\\n    --time 00:30:00 \\\n    /bin/bash -i\n</code></pre></p> <p>and this is an example <code>sbatch</code> command which will ask the cluster to run <code>my_script.sh</code> with 1 GPU and 10 CPUs, 10 GB of RAM that will run for at most 30 minutes (if the script has finished execution the job will be ended), the output and error logs will be saved to <code>log_JOBID.txt</code> (<code>--output</code>, <code>--error</code>): <pre><code>sbatch \\\n    --job-name pepe_run \\\n    --partition gpu \\\n    --gres gpu:1 \\\n    --mem-per-cpu 1g \\\n    --ntasks 1 \\\n    --cpus-per-task 10 \\\n    --time 00:30:00 \\\n    --output log_%j.txt \\\n    --error log_%j.txt \\\n    my_script.sh\n</code></pre> you may also use <code>--constraint='kepler|pascal|volta'</code> in order to select a specific gpu architecture.</p> <p>Instead of specifying the resources and other information as command-line arguments, you may find it useful to list them inside of <code>my_script.sh</code> and then just use <code>sbatch my_script.sh</code>: <pre><code>#!/bin/bash\n#SBATCH --job-name=pepe_run\n#SBATCH --gres=gpu:1\n#SBATCH --time=00:30:00\n# and so on. To comment SBATCH entry use `##SBATCH --arg ...`\n# here starts your script\n</code></pre></p> <p>To learn more <code>sbatch</code> hacks, a reader is also referred to this nice tutorial.</p>"},{"location":"Technical-Notes/tuni-narvi-cluster/#how-to-cancel-my-job","title":"How to Cancel My Job","text":"<p>To cancel a specific job you are running, use <pre><code>scancel &lt;JobID&gt;\n</code></pre></p>"},{"location":"Technical-Notes/tuni-narvi-cluster/#how-to-transfer-data","title":"How to Transfer Data?","text":"<p>The simplest way is to use <code>scp</code> command <pre><code>scp -i ~/.ssh/narvi_key -r ./folder user@narvi.tut.fi:/narvi/path/\n</code></pre> where <code>-r</code> means to copy the folder with all files in it.</p> <p>However, once the internet connection is interrupted you will need to start all over again. To have an opportunity to resume the data transfer try <code>rsync</code> instead <pre><code>rsync -ahP -e \"ssh -i ~/.ssh/narvi_key\" ./folder user@narvi.tut.fi:/narvi/path/\n</code></pre> where <code>-ah</code> means to preserve permissions symlinks, etc as in the original folder and <code>h</code> makes the progress \"human-readable\", and <code>P</code> allows to continue data transfer (sends missing files on the target path \ud83e\udd13).</p> <p>Trailing <code>/</code> in <code>rsync</code> makes the difference</p> <ul> <li><code>rsync /dir1/dir2/ /home/dir3</code> - copies the contents of <code>/dir1/dir2</code> but not the <code>dir2</code> folder itself.</li> <li><code>rsync /dir1/dir2 /home/dir3</code> \u2013 copies the folder <code>dir2</code> along with all its contents.</li> </ul> <p>If you would like to see the files from a remote machine you may mount the folder locally. On Ubuntu/Debian install <code>sshfs</code> and run this <pre><code>mkdir narvi_folder\nsshfs -o IdentityFile=~/.ssh/narvi_key user@narvi.tut.fi:/narvi/folder/ ./narvi_folder\n</code></pre> the content of the <code>/narvi/folder</code> will be shown in <code>./narvi_folder</code>. Mind that the changes in either folder will be reflected in another one.</p> <p>To unmount the folder use <pre><code>umount ./narvi_folder\n</code></pre></p>"},{"location":"Technical-Notes/tuni-narvi-cluster/#how-do-i-install-my-software","title":"How Do I Install My Software","text":"<p>Before you do so, check if the software you would like to install is already installed by the admin (e.g. matlab, cuda, and gcc). These are set up using <code>module</code> functionality. You can load a module by specifying <code>module load &lt;mod&gt;</code> inside of your script. To see all available modules run <code>module avail</code>.</p> <p>If you are not satisfied with the selection you can install your own. Here we will focus on <code>Python</code> packages and virtual environment manager <code>conda</code> which is already installed on Narvi (try: <code>which conda</code>).</p> <p><code>conda</code> Has Many Linux Tools</p> <p>Besides a ton of <code>Python</code> packages, <code>conda</code> has surprisingly many common Linux tools, e.g. <code>tmux</code>, <code>htop</code>, <code>ffmpeg</code>, <code>vim</code>, and more. This is especially useful if you would like to install them but do not have <code>sudo</code> rights.</p>"},{"location":"Technical-Notes/tuni-narvi-cluster/#creating-a-conda-environment","title":"Creating a <code>conda</code> Environment","text":"<p>Let's start by creating an empty conda environment <pre><code>conda create --name my_env\n</code></pre></p> <p>Activate it (meaning that all binaries installed in this environment will be used instead of the system-wise packages) <pre><code>conda activate my_env\n# if it didn't work try `source activate my_env`\n</code></pre></p> <p>Afterward, you can install <code>conda</code> packages <pre><code>conda install python pip matplotlib scikit-learn\n</code></pre></p> <p>If default <code>conda</code> channels don't have some package you search for other <code>conda</code> channels: <pre><code>conda install dlib --channel=menpo\n</code></pre></p> <p>If your favorite package is not available anywhere in <code>conda</code> OR you would like to install <code>OpenCV</code>, try to install it via <code>pip</code>: <pre><code># check if you are using the `pip` from your `conda` env\nwhich pip\npip install opencv-python\n</code></pre></p> <code>conda</code> vs <code>pip</code> inside of <code>conda</code> env? <p>According to official anaconda documentation, you should install as many requirements as possible with <code>conda</code>, then use <code>pip</code>. Another problem with <code>pip</code> packages inside of <code>conda</code> is associated with poor dependence handling and just bad experience when trying to replicate the same environment on another machine.</p>"},{"location":"Technical-Notes/version_control/","title":"Version Control","text":"<p>It is important that all your source code and documents are in version control since one day your computer will break down and that day you will thank yourself having them under version control. There are many other reasons, but that is the day when it really pays off.</p> <p>Below are instruction for various different purposes.</p>"},{"location":"Technical-Notes/version_control/#personal-version-control-using-university-ssh-servers","title":"Personal version control using university ssh servers","text":"<p>This is what you should do for your personal documents, such as CV, job applications, list of publications, love letters etc. that you feel uncomfortable to put on public servers such as GitHub, but that you wish to be safe, versioned and backup.</p> <p>For that purpose the university ssh servers and Subversion provide suitable tools. First, connect id.tuni.fi and obtain rights to use these services: \"manage your user rights\" -&gt; \"My user rights\" -&gt; \"Apply for a new user right\" -&gt; Choose student or staff contract -&gt; \"IT\" -&gt; \"Linux Servers\". Wait a few minutes and you should access to the servers.</p> <p>Create personal repository:</p> <pre><code>$ ssh linux-ssh.tuni.fi\n$ mkdir svn_repos; cd svn_repos\n$ mkdir &lt;personal_dir&gt;\n$ svnadmin create /home/&lt;my name&gt;/svn_repos/&lt;personal_dir&gt;\n</code></pre> <p>Now the repository is created and you can access it from your personal computer. First take out the repo:</p> <pre><code>$ cd Work\n$ svn co svn+ssh://&lt;my name&gt;@linux-ssh.tuni.fi/home/&lt;my name&gt;/svn_repos/&lt;personal_dir&gt;\n</code></pre> <p>Now you have personal repository that is stored and backup in the university system. SVN (Subversion) is pretty similar to Git, but simpler. For example, you don't \"push\" and \"pull\", but use \"svn commit\" to add you changes and \"svn update\" to update changes to the existing repository. </p> <p>A Fine SVN BOOK is available here http://svnbook.red-bean.com/</p>"},{"location":"Technical-Notes/version_control/#research-project-code-and-documents-in-github","title":"Research project code and documents in Github","text":"<p>Someone needs to write.</p>"},{"location":"Technical-Notes/wifi/","title":"Wireless connections","text":"<p>If you are tired to use your phone as a wifi hotspot you may try one of the university wireless networks.</p>"},{"location":"Technical-Notes/wifi/#tuni-staff","title":"TUNI-STAFF","text":"<p>Works in Windows and centrally maintained Linux boxes, but clearly making it available to self-maintained Linux boxes was beyond their skills. Do not try.</p>"},{"location":"Technical-Notes/wifi/#eduroam","title":"Eduroam","text":"<p>Eduroam provides access in many universities world wide and is also easiest wireless connection in TUNI premises.</p> <p>Easiest way to install Eduroam is to download a domain specific installation script  from https://cat.eduroam.org/ . Just follow the instructions and that's it!</p>"},{"location":"University-Bureaucracy/cs_curricula/","title":"Computing Sciences Curricula","text":"<p>The BSc and MSc degrees offered by Computing Sciences consist of 1) mandatory courses for everyone and 2) major module and 3) minor module(s). Below are details of each degree program offered by Computing Sciences.</p> <p>Below are links to relevant information in TAU Web pages:</p> <ul> <li>Search all courses, modules and programs of Tampere University (Note: language setting affects the search results)</li> </ul>"},{"location":"University-Bureaucracy/cs_curricula/#signal-processing-and-machine-learning-bsc-in-technology-finnish-program-30-cr-thesis","title":"Signal Processing and Machine Learning BSc in Technology (Finnish program, 30 cr + thesis)","text":"<p>The Signal Processing and Machine Learning (SPML) major is offered in the Bachelor of Science and Technology degree program of the Computing and Electrical Engineering. Our graduates are some of the most wanted in Finnish IT and EE companies and research institutions. The SPML major consists of three mandatory courses (tot 15 cr) and 15 cr from elective courses that can be selected from the list of suggested courses or by proposing a personal study plan.</p> <p></p>"},{"location":"University-Bureaucracy/cs_curricula/#signal-processing-and-machine-learning-msc-in-technology-finnish-program-30-cr-thesis","title":"Signal Processing and Machine Learning MSc in Technology (Finnish program, 30 cr + thesis)","text":"<p>Similar to our BSc program the MSc program also consists of two mandatory courses (10 cr) and then the student is encouraged to pick one of our three special sub-modules focusing on Audio, Vision or Artificial Intelligence. Since some of these sub-modules contain one shared course that course is moved to elective module.</p> <p></p>"},{"location":"University-Bureaucracy/employee/","title":"Make a work contract request","text":"<p>As a supervisor you need to confirm contracts to your group memers. For contracts send email to <code>itc-hr.tau at tuni.fi</code>. You need to mention:</p> <ul> <li>The period of contract (e.g. Jan 1 2021 - Dec 31 2021)</li> <li>The salary level (e.g. \"Starting PhD student\")</li> <li>Whether contract is full time (100%) or part-time</li> <li>From which project the salary is paid</li> </ul> <p>Also include the following persons to your email:</p> <ul> <li>Your controller</li> <li>Project leader (if not you)</li> </ul>"},{"location":"University-Bureaucracy/msc_thesis/","title":"MSc Thesis","text":"<p>MSc thesis project is the last step of your studies. It is typically six months of full-time coding, building, testing and writing. </p> <p>The thesis is about marketing yourself and therefore you should impress yourself, your family, your supervisors and your future employer.</p> <p>It is advisable to produce Github pages for your code and data with nice Wiki how to replicate the results and link to the thesis PDF. Its even better if the page also contains a Youtube video of your amazing work.</p>"},{"location":"University-Bureaucracy/msc_thesis/#stage-1-find-a-topic","title":"Stage 1 - Find a topic","text":"<p>When your studies are almost finished (2nd year of Master studies) you should start looking for a job or intern position where you can do your thesis. This could be at your current position, a university research group or perhaps you find a better place!</p> <p>You are young, full life ahead, so it is recommended that 1) you do something meaningful and difficult, 2) you learn a lot and 3) you impress people you work for.</p> <p>You could contact your professors and ask them for open topics in their research groups (paid and unpaid positions are available) or if they know any companies who are looking for a master's thesis worker. Be active and search until you find a place that suits you and you suit that place.</p>"},{"location":"University-Bureaucracy/msc_thesis/#stage-2-agree-about-the-topic-with-your-supervisors","title":"Stage 2 - Agree about the topic with your supervisors","text":"<p>Important: You need to understand what should be done in this thesis! You need to understand why this is an important topic (motivates that it needs to be done)! You must understand how to evaluate your results (otherwise it will be unclear what is the quality of your work).</p> <p>You should have two supervisors: 1) Academic supervisor who is a senior staff from university (professor, associate/assistant professor, lecturer etc. someone with doctoral degree) and 2)  a technical supervisor from the company you work for (preferably someone with at least MSc degree so that they know what MSc thesis is all about). You may interview multiple professors to find who is the most suitable for you. You know, there is a huge difference between supervisors and how much they have time and interest for you.</p> <p>Company pays your salary so you must make your technical supervisor to agree what you do, especially if you do the work during your working hours. You also must know confidential things that cannot be put to your thesis as MSc theses are always public.</p> <p>Concrete action: Fill and agree the thesis supervision plan with your supervisors (official form)</p>"},{"location":"University-Bureaucracy/msc_thesis/#stage-3-do-the-thesis-project-5-months","title":"Stage 3 - Do the thesis project (~5 months)","text":"<p>Important: There are three important things to bear in mind: 1) read what others have done (related work), 2) read what others have done (related work) and finally 3) read what others have done (related work).</p> <p>Before you can find the related works you must know the correct terminology of your problem! Only with the correct terms search engines (Google, you.com) can provide correct links to the existing code, articles and books.</p> <p>Steps:</p> <ul> <li>Find correct terminology (e.g. \"face verification\" vs. \"face recognition\" vs. \"face detection\")</li> <li>Search related works under this terminology - prefer recent works that have been published in good journals and conferences (ask your supervisor) and have been cited by others.</li> <li>Search existing code you can use</li> <li>Search existing data you can use</li> <li>Play with code and data to get your hands dirty</li> <li>Based on findings revise your topic and thesis plan</li> </ul> <p>Work hard, be diligent and consult your supervisors often! Yes, talking with your supervisors is your responsibility, not theirs.</p>"},{"location":"University-Bureaucracy/msc_thesis/#stage-4-write-thesis-1-month","title":"Stage 4 - Write thesis (~1 month)","text":"<p>This can happen parallel during stage 3, and its good to make notes all the time.</p> <p>Check out this Latex template: MSc thesis template (Latex)</p> <p>Examples of great theses (although your main supervisor may have different examples so ask him/her):</p> <ul> <li>Valtteri Kaatrasalo (2022): Computer vision methods for augmented reality</li> <li>Leevi Raivio (2021): Visual metric-semantic 3D reconstruction</li> <li>Lauri Suomela (2021): Implementing HR analytics: Premises for value creating analytics</li> <li>Atakan Dag (2021): Comparison of monolithic and hybrid controllers for multi-objective sim-to-real learning</li> <li>Eero Hein\u00e4nen (2018): A Method for automatic tuning of PID controller following Luus-Jaakola optimization</li> <li>Matti Tuhola (2018): English Lexical Stress Recognition Using Recurrent Neural Networks</li> <li>Giambattista Parascandolo (2015): Recurrent neural networks for polyphonic sound event detection</li> </ul> <p>Actions: You must attend the MSc thesis seminar course of you major. The seminar typically includes: 1) watching MSc presentations by others, 2) presenting your work (at least once, agree this with your supervisor); in CS the presentations are available in budjetti.cs.tut.fi, and 3) participating information literacy training by university Library. All details you will find from the course Moodle page or ask from the seminar course instructor.</p>"},{"location":"University-Bureaucracy/msc_thesis/#stage-5-submitting-the-thesis","title":"Stage 5 - Submitting the thesis","text":"<p>You must ask your academic supervisor comments for your thesis. Some supervisors comment multiple versions the manuscript, but some only the final draft i.e. the version that you think is pretty much ready. Ask your supervisor(s) what he/she prefers. Remember that you are evaluated every time you send something to your supervisor!</p> <p>After the supervisors are happy to the current version:</p> <ul> <li>Submit thesis to Turnitin originality test (your supervisor's Moodle space) official instructions</li> <li>After the Turnitin  if fine, then submit the final version to Trepo and let your supervisor know it is there for his/her evaluation official instructions</li> </ul> <p>Evaluation criteria and evaluation templates can be found from thise official page</p>"},{"location":"University-Bureaucracy/msc_thesis/#freedom","title":"Freedom","text":"<p>Enjoy your life, You deserve good life!</p> <p>However, keep updating your knowledge so that your knowledge and skills remain curant for the future jobs and needs!</p>"},{"location":"University-Bureaucracy/phd_thesis/","title":"PhD Thesis","text":"<p>So, you decided that doing a PhD thesis is good for your future. Along the long and winding road you may find the following instructions useful.</p>"},{"location":"University-Bureaucracy/phd_thesis/#stage-0-find-supervisor-and-apply-for-doctoral-studies-year-0","title":"Stage 0 - Find supervisor and apply for doctoral studies (year 0)","text":"<p>You must be absolutely sure that you want a PhD degree since it means four years of super heavy work under (possibly) monstrous supervisor. On the positive side, you will learn how to make science and that opens the door to the academic career. In addition, many R&amp;D labs of big companies appreciate PhD degree from their employees.</p> <p>In order to apply you need to fill and submit a number of super boring bureaucratic forms that insist your commitment and also commitment from your main supervisor (make sure you found a good one):</p> <ul> <li>Forms to be submitted for doctoral studies</li> </ul> <p>Before applying send an email to doctoral studies office and ask for the list of things to be submitted and a link to the electronic application system.</p> <p>Welcome, you just started your jorney toward magic of science.</p>"},{"location":"University-Bureaucracy/phd_thesis/#stage-1-do-research-years-1-4","title":"Stage 1 - Do research (years 1-4)","text":"<p>Input: 1) you, 2) your supervisor, and 3) funding for your salary and research.</p> <p>This is the most challenging part, but don't worry, your supervisor will tell you all the necessary details and tricks. However, you may find the following links helpful or at least fun to read:</p> <ul> <li>A Survival Guide to a PhD by Andrej Karpahty</li> <li>How to do research by Bill Freeman</li> <li>Scientific article writing by Fredo Durand</li> </ul> <p>Output: Novel scientific knowledge. More concretely, a number of peer-reviewed scientific articles.</p>"},{"location":"University-Bureaucracy/phd_thesis/#stage-2-write-the-thesis-manuscript-year-4","title":"Stage 2 - Write the thesis manuscript (year 4)","text":"<p>Input: Sufficient amount of scientific contributions, typically in the terms of the articles from the previous stage.</p> <p>It is difficult to tell a generic rule when you're ready to start writing your thesis as that varies a lot between the fields, even between fields close to each other. You should discuss with your supervisor each year what is the stage of your PhD studies and she/he certainly tells you when it is time to start writing.</p> <p>There are recommendations for PhD thesis from DPCEE. It is worth to read through and perhaps adopt their structure, but of course you should follow practices in your field and discuss with your supervisor. However, it seems that the committee provides a lot of feedback if you do not follow these recommendations.</p> <p>Before writing you need a suitable template. Vast majority of people prefer Latex as it is just so convenient for writing scientific text.</p> <ul> <li>A popular template is provided by Ville Koljonen: https://www.overleaf.com/read/zskzfgpnpjcz</li> </ul> <p>Output: A thesis manuscript that summarizes your research and results. The thesis can be a monograph or a compilation thesis that includes your original articles. Write using a meaningful structure and beautiful grammar and send it for comments to your supervisor(s) before proceeding to the next stage.</p>"},{"location":"University-Bureaucracy/phd_thesis/#stage-3-pre-examination","title":"Stage 3 - Pre-examination","text":"<p>Input: Final draft of thesis and fixed using the comments from your supervisor. The pre-examination form.</p> <p>Your supervisor selects two reviewers, makes sure they are available and willing and fills the official form (see below). You need to send the following two documents to your doctoral program academic officer (contact details and submission deadlines):</p> <ol> <li>The \"proposal of pre-examiners\" form filled (ask your supervisor to help).</li> <li>PhD thesis in PDF format. If a compilation thesis, then it should include the articles. Make a single PDF and if its size &gt;15MB you should downscale your images before compiling the PDF</li> </ol> <p>You may also want to read general guidelines from the university.</p> <p>Your documents will be processed by the Faculty council (they meet ~once a month) and you will receive an email notification once this is done. Your pre-examiners will also receive a formal invitation and the submitted PhD thesis. Now you cross your fingers, wait for two months and hope for positive reviews. Time to think what you will do next in your life and perhaps stat to look for a position in academia or industry!</p> <p>Output: Faculty council decision for pre-examination. Official invitations to the pre-examiners.</p>"},{"location":"University-Bureaucracy/phd_thesis/#stage-4-defense","title":"Stage 4 - Defense","text":"<p>Input: 1) positive statement from the pre-examiners. Form to publish the thesis.</p> <p>Your supervisor selects 1-2 opponents who will come and publicly \"torture\" you, i.e. ask nasty questions about your thesis. You also need to publish your thesis, i.e. make it publicly and openly available. Submit the following documents to the Academic Officer of your doctoral program:</p> <ol> <li>The \"Application to apply the right to publish the dissertation\" form filled.</li> <li>A document describing the changes requested by the pre-examiners (if requested) -- see the bottom of the \"Application to apply the right to publish the dissertation\" form. This document is similar to \"Revision letter\" where you reply to reviewers' comments if they requested \"major revision\" for your article. If necessary discuss with your supervisor how to do these.</li> <li>The \"Proposal of opponent(s), custos and the public defence date\" form filled (written by your supervisor, and signed by both of you).</li> </ol> <p>Again the official permission comes from the Faculty council, but meanwhile you already prepare the following:</p> <ul> <li>You send the thesis and a separate title page document (tempate at https://libguides.tuni.fi/dissertationpublishing/ ) to dissertations \u00e4t tuni.fi</li> <li>You send the title page, thesis and articles (if separate from the thesis PDF) to Punamusta for printing using this form https://tilaukset.omapumu.com/tampereen_yliopisto/index.php?kielivalinta=englanti - your PhD programme coordinator will tell you the minimum amount to be printed and paid by he faculty. You might want to pay a few more copies for your personal use.</li> </ul> <p>After Faculty council informs you that the permission to publish is granted:</p> <ul> <li>immediately notify dissertations \u00e4t tuni.fi to publish the thesis in Trepo (example: https://trepo.tuni.fi/handle/10024/124060)</li> <li>Your supervisor should send the link to the thesis to the opponent(s)</li> <li>immediately notify dissertations \u00e4t punamusta.com to start printing</li> <li>You should send printed books to the opponents as well</li> </ul> <p>Output: 1) permission for the public defense, 2) public link to your thesis in the library database, 3) printed thesis, 4) press release and 5) public defense.</p>"},{"location":"University-Bureaucracy/phd_thesis/#pre-defense-pr","title":"Pre-defense - PR","text":"<p>You are almost done. Next you need just a little bit of marketing and planning.</p> <ul> <li>Press release use the template at https://libguides.tuni.fi/dissertationpublishing/pressrelease along with a nice picture of you and send to viestinta.tau \u00e4t tuni.fi - typically your supervisor takes care of this</li> <li>Book defense auditorium (if physical defense)</li> <li>Order coffee for the defense (if physical defense)</li> <li>Prepare Zoom meeting (if online) and test everything</li> </ul>"},{"location":"University-Bureaucracy/phd_thesis/#in-defense-the-defense-act","title":"In-defense - The defense act","text":"<p>This is a formal \"act\" with pre-defined lyrics for custos, candidate and opponents. More details available at</p> <ul> <li>https://intra.tuni.fi/en/handbook?page=4231</li> </ul>"},{"location":"University-Bureaucracy/phd_thesis/#post-defense-karonkka-dinner","title":"Post-defense -  \"Karonkka\" dinner","text":"<p>This is not an official part of your defense, but expected in Finland. That means that to \"thank\" your opponents you will take them for a formal table served dinner to some nice place. During Karonkka, typically during the coffee, there are speeches where you thank people who contributed to your successful PhD thesis (semi-official order: opponents, supervisor, co-authors, others)</p>"},{"location":"University-Bureaucracy/phd_thesis/#freedom-years-5-128","title":"Freedom (years 5-128)","text":"<p>Enjoy your life (finally ;-) !</p>"}]}